<<<<<<< HEAD
# Weather ETL Pipeline (SHIFT 2025)
## О проекте

ETL-пайплайн для работы с данными о погоде, разработанный как тестовое задание для курса Data Engineer SHIFT. Проект выполняет:

- **Загрузку** данных с Open-Meteo API
- **Трансформацию** (конвертация единиц, агрегация)
- **Сохранение** в CSV или PostgreSQL


## Технологический стек

| Компонент       | Выбор                          | Обоснование                                                                 |
|-----------------|--------------------------------|-----------------------------------------------------------------------------|
| **Язык**       | Python 3.8+                   | Простота обучения, богатая экосистема для Data Engineering                  |
| **База данных**| PostgreSQL 13                 | Бесплатная, популярная в индустрии, хорошая поддержка географических данных|
| **Оркестрация**| Docker                        | Упрощение развертывания БД без локальной установки                          |


## Как запустить (шаг за шагом)
1. Установите Docker
2. Запустите базу данных:
   ```bash
   docker-compose up -d
3. Установите зависимости:
   ```bash
   pip install pandas requests psycopg2-binary python-dotenv
4. Запустите программу:
   ```bash
   python python_etl.py
5. Программа попросит задать:
- Дату начала и конца (в формате ГГГГ-ММ-ДД)
- Куда сохранить результат (CSV, БД или оба варианта)


## Трансформация данных
Автоматическая конвертация в метрическую систему:

- Температура: °F → °C
- Скорость ветра: узлы → м/с
- Осадки: дюймы → мм


## Борьба с дубликатами
Как это работает:
При попытке вставить новые данные, если запись с такой датой уже есть - PostgreSQL обновляет старые данные новыми и не создаёт дублей

   ```bash
   INSERT INTO weather (...) 
   ON CONFLICT (date) DO UPDATE ...

Почему именно так?
- Проще чем удалять старые записи вручную
- Надёжнее чем игнорировать новые данные
- Быстрее чем проверять вручную перед каждой вставкой

## Решение проблемы с часовыми поясами
- Проблема: При запросе данных через API время учитывалось в UTC, что приводило к смещению дат на 1 день назад для нашего часового пояса (например, данные за 1 мая могли отображаться как 30 апреля).

Моё решение:

   ```bash
   # Расширяем запрашиваемый период на 2 дня
   load_start_date = (requested_start_date - timedelta(days=1)).strftime("%Y-%m-%d")
   load_end_date = (requested_end_date + timedelta(days=1)).strftime("%Y-%m-%d")
   
   # После получения данных фильтруем по нужному диапазону
   filtered_data = data[(data['date'] >= requested_start_date) & 
                     (data['date'] <= requested_end_date)]
Почему именно так?
- Простота - не нужно сложных преобразований времени
- Надёжность - гарантированно получаем все нужные данные
- Гибкость - работает для любого часового пояса


## Инструкция по запуску проекта
1. Установка необходимого ПО:
Docker Desktop
Python 3.8+
Git

2. Клонирование репозитория
Откройте терминал (CMD/PowerShell на Windows) и выполните:

   ```bash
   git clone https://github.com/angelikalenykh/shift_task.git
   cd shift_task
   
3. Запуск базы данных

   ```bash
   docker-compose up -d
   
Это запустит PostgreSQL в фоновом режиме

4. Установка зависимостей Python

   ```bash
   pip install -r requirements.txt
   
Если возникает ошибка, попробуйте:

   ```bash
   python -m pip install -r requirements.txt
   
5. Запуск ETL-процесса

   ```bash
   python python_etl.py
   
6. Ввод параметров
Программа попросит указать:

Начальную дату (формат: ГГГГ-ММ-ДД)
Конечную дату
Способ сохранения (CSV/БД/Оба варианта)
=======
# de_shift_task
>>>>>>> 27ec0d05aa635c7d5ef3467c97a43a0d2b5f0552
